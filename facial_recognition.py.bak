import cv2
import os
import pyodbc
import logging
import base64
import uuid
import pickle
import numpy as np
import tensorflow as tf
import mxnet as mx
from datetime import datetime
from pymongo import MongoClient
from gridfs import GridFS
from mtcnn_detector import MtcnnDetector
import facenet
from sklearn.metrics.pairwise import cosine_similarity
from PIL import Image
import io

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('facial_recognition.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    # SQL Server Configuration
    "sql_server": "127.0.0.1",
    "sql_db": "patiyala",
    "sql_username": "SA",
    "sql_password": "eRISEpLeHa",
    
    # MongoDB Configuration
    "mongodb_uri": "mongodb://localhost:27017/",
    "mongodb_name": "PATIALA_2025-01-30",
    
    # Processing Parameters
    "batch_size": 500,
    "num_workers": 8,
    "similarity_threshold": 0.7,
    
    # Collection Names
    "collections": {
        "bib_recognition": "bib_detection_results",
        "wrong_tag_results": "wrong_tag_results",
        "facial_results": "facial_recognition_results",
        "facial_detections": "facial_detection_details",
        "images": "images"
    }
}

# Global models
global_models = {
    'detector': None,
    'session': None,
    'graph': None
}

def init_models():
    """Initialize face detection and recognition models"""
    try:
        print("Initializing face detection model...")
        global_models['detector'] = MtcnnDetector(
            model_folder='model',
            ctx=mx.cpu(1),
            num_worker=4,
            accurate_landmark=False
        )
        
        print("Initializing face recognition model...")
        tf_config = tf.compat.v1.ConfigProto()
        tf_config.gpu_options.allow_growth = True
        global_models['session'] = tf.compat.v1.Session(config=tf_config)
        
        with global_models['session'].as_default():
            with tf.compat.v1.gfile.GFile('facial_embeddings_pet.pb', 'rb') as f:
                graph_def = tf.compat.v1.GraphDef()
                graph_def.ParseFromString(f.read())
                tf.import_graph_def(graph_def, name='')
        
        global_models['graph'] = tf.compat.v1.get_default_graph()
        
        print("All models initialized successfully")
        return True
    except Exception as e:
        logger.error(f"Model initialization failed: {str(e)}")
        return False

def create_db_connection():
    """Create SQL Server connection"""
    try:
        conn_str = (
            f"DRIVER={{SQL Server}};"
            f"SERVER={CONFIG['sql_server']};"
            f"DATABASE={CONFIG['sql_db']};"
            f"UID={CONFIG['sql_username']};"
            f"PWD={CONFIG['sql_password']}"
        )
        return pyodbc.connect(conn_str)
    except Exception as e:
        logger.error(f"Database connection failed: {str(e)}")
        return None

def get_mongo_client():
    """Create MongoDB client"""
    try:
        return MongoClient(CONFIG['mongodb_uri'])
    except Exception as e:
        logger.error(f"MongoDB connection failed: {str(e)}")
        return None

def get_face_embedding(face_img):
    """Get embedding for a single face"""
    try:
        with global_models['graph'].as_default():
            with global_models['session'].as_default():
                resized = cv2.resize(face_img, (160, 160))
                prewhitened = facenet.prewhiten(resized)
                feed_dict = {
                    'input:0': [prewhitened],
                    'phase_train:0': False
                }
                return global_models['session'].run('embeddings:0', feed_dict=feed_dict)
    except Exception as e:
        logger.error(f"Error getting face embedding: {str(e)}")
        return None

def extract_faces(image):
    """Extract all faces from an image"""
    try:
        results = global_models['detector'].detect_face(image)
        if results is None:
            return [], []
            
        total_boxes = results[0]
        points = results[1]
        chips = global_models['detector'].extract_image_chips(image, points, 160, 0.33)
        return chips, total_boxes.tolist()
    except Exception as e:
        logger.error(f"Error extracting faces: {str(e)}")
        return [], []

def decode_pet_regimage(img_data):
    """Decode PET registration image from SQL"""
    try:
        if isinstance(img_data, bytes):
            img = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_COLOR)
            if img is None:
                raise ValueError("Failed to decode image bytes")
        else:
            img_data = img_data.replace(" ", "+")
            if ',' in img_data:
                img_data = img_data.split(',')[-1]
            img = cv2.cvtColor(
                np.array(Image.open(io.BytesIO(base64.b64decode(img_data)))),
                cv2.COLOR_BGR2RGB
            )
        return img
    except Exception as e:
        logger.error(f"Error decoding PET image: {str(e)}")
        return None

def retriving_petimage(image_id):
    """Retrieve PET image from MongoDB GridFS with enhanced error handling"""
    try:
        client = get_mongo_client()
        if client is None:
            return None
            
        db = client[CONFIG['mongodb_name']]
        fs = GridFS(db, CONFIG['collections']['images'])
        
        img_file = fs.get(image_id)
        if img_file is None:
            logger.error(f"Image {image_id} not found in GridFS")
            return None
            
        img_data = img_file.read()
        if not img_data:
            logger.error(f"Empty image data for {image_id}")
            return None
            
        # Try multiple decoding methods
        img = None
        try:
            # First try standard decoding
            img = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_COLOR)
            if img is None:
                # Try alternative decoding if first attempt fails
                nparr = np.frombuffer(img_data, np.uint8)
                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
            if img is None:
                # Final fallback using PIL
                pil_image = Image.open(io.BytesIO(img_data))
                img = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
                
            if img is None:
                raise ValueError("All image decoding methods failed")
                
            return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
        except Exception as decode_error:
            logger.error(f"Failed to decode image {image_id}: {str(decode_error)}")
            return None
            
    except Exception as e:
        logger.error(f"Error retrieving image {image_id}: {str(e)}")
        return None
    finally:
        if 'client' in locals():
            client.close()

def get_frames_to_process():
    """Get frames from both collections with proper filtering"""
    try:
        client = get_mongo_client()
        if client is None:
            return []
            
        db = client[CONFIG['mongodb_name']]
        
        # Query for bib_detection_results
        bib_query = {
            "image": {"$exists": True, "$ne": None},
            "$or": [
                {"facial_flag": {"$exists": False}},
                {"facial_flag": None},
                {"facial_flag": ""}
            ],
            "rollno": {"$exists": True, "$ne": None}
        }
        
        # Query for wrong_tag_results (less strict requirements)
        wrong_query = {
            "image": {"$exists": True, "$ne": None},
            "$or": [
                {"facial_flag": {"$exists": False}},
                {"facial_flag": None},
                {"facial_flag": ""}
            ]
        }
        
        frames = []
        
        # Process bib_detection_results
        bib_coll = db[CONFIG['collections']['bib_recognition']]
        for doc in bib_coll.find(bib_query):
            doc['_collection'] = 'bib_detection_results'
            frames.append(doc)
        
        # Process wrong_tag_results
        wrong_coll = db[CONFIG['collections']['wrong_tag_results']]
        for doc in wrong_coll.find(wrong_query):
            doc['_collection'] = 'wrong_tag_results'
            frames.append(doc)
        
        logger.info(f"Found {len(frames)} frames to process (from both collections)")
        return frames
    except Exception as e:
        logger.error(f"Error getting frames to process: {str(e)}")
        return []
    finally:
        if 'client' in locals():
            client.close()

def process_pet_images():
    """Process all PET images from SQL and save embeddings"""
    try:
        conn = create_db_connection()
        if conn is None:
            return False
            
        cursor = conn.cursor()
        cursor.execute("SELECT roll_no, imagedata_face FROM dbo.imagedata WHERE imagedata_face IS NOT NULL")
        
        pet_data = {}
        print("Processing PET images...")
        processed_count = 0
        failed_count = 0
        
        for roll_no, img_data in cursor:
            try:
                img = decode_pet_regimage(img_data)
                if img is None:
                    failed_count += 1
                    continue
                    
                faces, _ = extract_faces(img)
                if faces:
                    embedding = get_face_embedding(faces[0])
                    if embedding is not None:
                        pet_data[str(roll_no)] = embedding[0]
                        processed_count += 1
            except Exception as e:
                failed_count += 1
                logger.error(f"Error processing PET {roll_no}: {str(e)}")
        
        with open('pet_embeddings.pkl', 'wb') as f:
            pickle.dump(pet_data, f)
            
        logger.info(f"Saved {processed_count} PET embeddings ({failed_count} failed)")
        return True
    except Exception as e:
        logger.error(f"Error in process_pet_images: {str(e)}")
        return False
    finally:
        if 'conn' in locals():
            conn.close()

def process_mongo_frames():
    """Process all frames from MongoDB and save embeddings"""
    try:
        frames = get_frames_to_process()
        if not frames:
            logger.info("No frames to process")
            return False
            
        frame_data = {}
        print("Processing frames from MongoDB...")
        processed_count = 0
        failed_count = 0
        
        for doc in frames:
            try:
                img = retriving_petimage(doc['image'])
                if img is None:
                    failed_count += 1
                    logger.warning(f"Skipping frame {doc['_id']} - image could not be loaded")
                    continue
                    
                faces, bboxes = extract_faces(img)
                if faces:
                    embeddings = []
                    for face in faces:
                        emb = get_face_embedding(face)
                        if emb is not None:
                            embeddings.append(emb[0])
                    
                    if embeddings:
                        frame_data[str(doc['_id'])] = {
                            'embeddings': embeddings,
                            'bboxes': bboxes,
                            'metadata': {
                                'rollno': doc.get('rollno'),
                                'bib': doc.get('final_verified_bib') or doc.get('bib_number', ''),
                                'timestamp': doc.get('timestamp'),
                                'lap': doc.get('Lap'),
                                'source_collection': doc.get('_collection', 'unknown'),
                                'ai_unique_id': doc.get('ai_unique_id', '')
                            }
                        }
                        processed_count += 1
            except Exception as e:
                failed_count += 1
                logger.error(f"Error processing frame {doc.get('_id')}: {str(e)}")
        
        with open('frame_embeddings.pkl', 'wb') as f:
            pickle.dump(frame_data, f)
            
        logger.info(f"Saved {processed_count} frame embeddings ({failed_count} failed)")
        return True
    except Exception as e:
        logger.error(f"Error in process_mongo_frames: {str(e)}")
        return False

def update_facial_flags(frame_ids):
    """Update facial_flag in both collections for processed frames"""
    try:
        client = get_mongo_client()
        if client is None:
            return False
            
        db = client[CONFIG['mongodb_name']]
        
        # Split into batches
        for i in range(0, len(frame_ids), CONFIG['batch_size']):
            batch = frame_ids[i:i + CONFIG['batch_size']]
            
            # Update bib_detection_results
            bib_result = db[CONFIG['collections']['bib_recognition']].update_many(
                {'_id': {'$in': batch}},
                {'$set': {'facial_flag': '1'}}
            )
            
            # Update wrong_tag_results
            wrong_result = db[CONFIG['collections']['wrong_tag_results']].update_many(
                {'_id': {'$in': batch}},
                {'$set': {'facial_flag': '1'}}
            )
            
            logger.info(
                f"Updated batch {i//CONFIG['batch_size'] + 1}: "
                f"Bib={bib_result.modified_count}, Wrong={wrong_result.modified_count}"
            )
        
        return True
    except Exception as e:
        logger.error(f"Error updating facial flags: {str(e)}")
        return False
    finally:
        if 'client' in locals():
            client.close()

def match_faces():
    """Match PET faces with frames using pre-computed embeddings"""
    try:
        # Load embeddings
        print("Loading embeddings...")
        try:
            with open('pet_embeddings.pkl', 'rb') as f:
                pet_embeddings = pickle.load(f)
            
            with open('frame_embeddings.pkl', 'rb') as f:
                frame_embeddings = pickle.load(f)
        except Exception as e:
            logger.error(f"Error loading embeddings: {str(e)}")
            return False
        
        client = get_mongo_client()
        if client is None:
            return False
            
        db = client[CONFIG['mongodb_name']]
        
        results = []
        detection_details = []
        processed_frame_ids = []
        
        print("Matching faces...")
        matched_count = 0
        not_matched_count = 0
        
        for frame_id, frame_data in frame_embeddings.items():
            frame_rollno = frame_data['metadata']['rollno']
            if frame_rollno not in pet_embeddings:
                continue
                
            # Vectorized similarity calculation
            similarities = cosine_similarity(
                [pet_embeddings[frame_rollno]],
                frame_data['embeddings']
            )
            best_idx = np.argmax(similarities)
            best_sim = similarities[0][best_idx]
            
            match_status = 'matched' if best_sim > CONFIG['similarity_threshold'] else 'not_matched'
            if match_status == 'matched':
                matched_count += 1
            else:
                not_matched_count += 1
            
            # Prepare result document
            result_doc = {
                'frame_id': frame_id,
                'rollno': frame_rollno,
                'bib_number': frame_data['metadata']['bib'],
                'similarity': float(best_sim),
                'match_status': match_status,
                'source_collection': frame_data['metadata'].get('source_collection', 'unknown'),
                'processed_at': datetime.now(),
                'lap': frame_data['metadata']['lap'],
                'pet_date': frame_data['metadata']['timestamp'].split(' ')[0] if frame_data['metadata']['timestamp'] else None,
                'ai_unique_id': frame_data['metadata'].get('ai_unique_id', ''),
                'original_doc': {
                    'bib_number': frame_data['metadata']['bib'],
                    'rollno': frame_rollno,
                    'timestamp': frame_data['metadata']['timestamp'],
                    'Lap': frame_data['metadata']['lap']
                }
            }
            results.append(result_doc)
            processed_frame_ids.append(frame_id)
            
            # Prepare detection details
            detection_details.append({
                'frame_id': frame_id,
                'rollno': frame_rollno,
                'bib_number': frame_data['metadata']['bib'],
                'pet_date': frame_data['metadata']['timestamp'].split(' ')[0] if frame_data['metadata']['timestamp'] else None,
                'timestamp': frame_data['metadata']['timestamp'],
                'Lap': frame_data['metadata']['lap'],
                'no_of_faces_detected': len(frame_data['embeddings']),
                'best_match_similarity': float(best_sim),
                'best_match_bbox': frame_data['bboxes'][best_idx],
                'source_collection': frame_data['metadata'].get('source_collection', 'unknown'),
                'ai_unique_id': frame_data['metadata'].get('ai_unique_id', ''),
                'processed_at': datetime.now()
            })
        
        # Create new collections if they don't exist
        if CONFIG['collections']['facial_results'] not in db.list_collection_names():
            db.create_collection(CONFIG['collections']['facial_results'])
        
        if CONFIG['collections']['facial_detections'] not in db.list_collection_names():
            db.create_collection(CONFIG['collections']['facial_detections'])
        
        # Bulk insert results
        if results:
            print(f"Inserting {len(results)} facial recognition results ({matched_count} matched, {not_matched_count} not matched)...")
            db[CONFIG['collections']['facial_results']].insert_many(results)
            logger.info(f"Inserted {len(results)} facial recognition results")
        
        if detection_details:
            print(f"Inserting {len(detection_details)} facial detection details...")
            db[CONFIG['collections']['facial_detections']].insert_many(detection_details)
            logger.info(f"Inserted {len(detection_details)} facial detection details")
        
        # Update flags in both source collections
        if processed_frame_ids:
            print("Updating facial flags in source collections...")
            update_facial_flags(processed_frame_ids)
        
        return True
    except Exception as e:
        logger.error(f"Error in match_faces: {str(e)}")
        return False
    finally:
        if 'client' in locals():
            client.close()

def main():
    """Main execution pipeline"""
    if not init_models():
        exit(1)
    
    try:
        print("Starting facial recognition pipeline")
        
        print("Processing PET images from SQL...")
        if not process_pet_images():
            print("PET image processing failed")
            exit(1)
        
        print("Processing frames from MongoDB...")
        if not process_mongo_frames():
            print("Frame processing failed")
            exit(1)
        
        print("Matching faces...")
        if not match_faces():
            print("Face matching failed")
            exit(1)
        
        print("Pipeline completed successfully")
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
    finally:
        # Clean up
        if global_models['session']:
            global_models['session'].close()

if __name__ == "__main__":
    main()